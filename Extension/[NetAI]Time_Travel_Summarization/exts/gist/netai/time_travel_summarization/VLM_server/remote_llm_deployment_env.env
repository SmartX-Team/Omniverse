export NGC_API_KEY=nvapi-*** #FIXME - api key to pull model from NGC. Should come from ngc.nvidia.com
export NVIDIA_API_KEY=nvapi-*** #api key to access NIM endpoints. Should come from build.nvidia.com

#Adjust ports if needed
export FRONTEND_PORT=9100
export BACKEND_PORT=8100

#Change default user and pass if needed 
export GRAPH_DB_USERNAME=neo4j
export GRAPH_DB_PASSWORD=password

#Update paths local paths to config files if needed. If it appears VSS is not using these configurations, then change the relative paths to absolute paths. 
export DISABLE_CA_RAG=true
export CA_RAG_CONFIG=./config.yaml
export GUARDRAILS_CONFIG=./guardrails

export ENABLE_AUDIO=false

export RIVA_ASR_SERVER_URI="grpc.nvcf.nvidia.com"
export RIVA_ASR_GRPC_PORT=443
export RIVA_ASR_SERVER_IS_NIM=true
export RIVA_ASR_SERVER_USE_SSL=true
export RIVA_ASR_SERVER_API_KEY=nvapi-*** #FIXME - api key RIVA ASR server
export RIVA_ASR_SERVER_FUNC_ID="d8dd4e9b-fbf5-4fb0-9dba-8cf436c8d965"

# Error fixing
export TORCHINDUCTOR_DISABLE_AUTOTUNE=1
export VLLM_GPU_MEMORY_UTILIZATION=0.6
export VLLM_ATTENTION_BACKEND=flashinfer
export VLLM_USE_TRITON_FLASH_ATTENTION=0
export CUDA_LAUNCH_BLOCKING=1 
export TRITON_AUTOTUNE_POINTWISE=0
export TRITON_AUTOTUNE_CONV=0
export TRITON_MAX_CL_SIZE=64
export TRITON_MAX_CL_NUM_WARPS=4
export VLLM_DISABLE_TORCH_COMPILE=1
export TORCH_COMPILE_DISABLE=1  
export TORCHINDUCTOR_DISABLE=1
export VLLM_COMPILATION_LEVEL=0
export VLLM_USE_FLASHINFER=0
export VLLM_SAMPLING_USE_FLASHINFER=0
export VLLM_ATTENTION_BACKEND=triton
export VLLM_USE_TRITON_FLASH_ATTENTION=1

export DISABLE_CV_PIPELINE=true
export INSTALL_PROPRIETARY_CODECS=false # Set to true when enabling CV


# model selection - uncomment the model you want to use

#Set VLM to Cosmos-Reason1
# export VLM_MODEL_TO_USE=cosmos-reason1
# export MODEL_PATH=git:https://huggingface.co/nvidia/Cosmos-Reason1-7B
# export NVIDIA_VISIBLE_DEVICES=2


# Set VLM to VILA 1.5 40B
# export VLM_MODEL_TO_USE=vila-1.5
# export MODEL_PATH=ngc:nim/nvidia/vila-1.5-40b:vila-yi-34b-siglip-stage3_1003_video_v8 
# export MODEL_ROOT_DIR=/home/netai/wonjune/models
# export MODEL_PATH=/home/netai/wonjune/models/vila-1.5-40b/vila-1.5-40b_vvila-yi-34b-siglip-stage3_1003_video_v8
# export NVIDIA_VISIBLE_DEVICES=2


# Set VLM to NVILA 15B Highres
# export VLM_MODEL_TO_USE=nvila
# export MODEL_PATH=ngc:nvidia/tao/nvila-highres:nvila-lite-15b-highres-lita 
# export NVIDIA_VISIBLE_DEVICES=2


#Set VLM to Qwen3-VL-8B 
export VLM_MODEL_TO_USE=openai-compat
export OPENAI_API_KEY="empty"
export VIA_VLM_ENDPOINT="http://host.docker.internal:38011/v1"
export VIA_VLM_OPENAI_MODEL_DEPLOYMENT_NAME="Qwen3-VL-8B-Instruct" 
export NVIDIA_VISIBLE_DEVICES=2 # VSS pipeline이 청킹 및 디코딩을 하는 GPU 설정
# 짧은 영상 (1분) 의 경우에는 VSS pipeline과 VLM이 같은 GPU를 써도 무방. 긴 영상의 경우에는 서로 다른 GPU를 쓰는 것을 권장.



#Adjust misc configs if needed
export DISABLE_GUARDRAILS=true
# export NVIDIA_VISIBLE_DEVICES=0 #For H100 Deployment
# export NVIDIA_VISIBLE_DEVICES=0,1 #For L40S Deployment
# export TRT_LLM_MODE=int8 #nila??

# export VLLM_TENSOR_PARALLEL_SIZE=2
export NUM_VLM_PROCS=4
# export VSS_NUM_GPUS_PER_VLM_PROC=1
# export VLLM_TENSOR_PARALLEL_SIZE=1
# export VLLM_PIPELINE_PARALLEL_SIZE=1
# export VLM_CLEAR_CACHE_BETWEEN_REQUESTS=1
# export VLM_UNIQUE_TMP_PER_PROC=1         # 프로세스별 고유 tmp 디렉토리
# export VLM_TMP_BASE=/dev/shm/vlm_tmp     # 충분히 큰 shm 경로 지정
# export VLM_VIDEO_FILE_LOCK=1


# export VLM_DEFAULT_NUM_FRAMES_PER_CHUNK=8
# export VLM_DISABLE_FFMPEG_POOL=1           # 청크마다 ffmpeg 새로 열기
# export VLM_FORCE_SYNC_FRAME_LOAD=1         # 프레임 로딩 동기화, non-async
# export VLM_CLOSE_VIDEO_AFTER_CHUNK=1       # 청크 끝날 때마다 파일 핸들/파이프 닫기


# log 설정 (적당한 경로로 변경)
# export VSS_LOG_LEVEL=DEBUG
# export VIA_LOG_DIR=/home/netai/wonjune/timetravel/video-search-and-summarization/deploy/docker/remote_llm_deployment/logs
