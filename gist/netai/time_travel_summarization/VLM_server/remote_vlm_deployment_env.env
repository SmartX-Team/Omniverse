export OPENAI_API_KEY=sk-proj-***
export NGC_API_KEY=nvapi-*** #FIXME - api key to pull model from NGC. Should come from ngc.nvidia.com
export NVIDIA_API_KEY=nvapi-*** #api key to access NIM endpoints. Should come from build.nvidia.com


#Adjust ports if needed
export FRONTEND_PORT=9100
export BACKEND_PORT=8100

#Change default user and pass if needed 
export GRAPH_DB_USERNAME=neo4j
export GRAPH_DB_PASSWORD=password

#Update paths local paths to config files if needed. If it appears VSS is not using these configurations, then change the relative paths to absolute paths. 
export DISABLE_CA_RAG=true
export CA_RAG_CONFIG=./config.yaml
export GUARDRAILS_CONFIG=./guardrails

export ENABLE_AUDIO=false

export DISABLE_CV_PIPELINE=true
export INSTALL_PROPRIETARY_CODECS=false # Set to true when enabling CV

export RIVA_ASR_SERVER_URI="grpc.nvcf.nvidia.com"
export RIVA_ASR_GRPC_PORT=443
export RIVA_ASR_SERVER_IS_NIM=true
export RIVA_ASR_SERVER_USE_SSL=true
export RIVA_ASR_SERVER_API_KEY=nvapi-***
export RIVA_ASR_SERVER_FUNC_ID="d8dd4e9b-fbf5-4fb0-9dba-8cf436c8d965"

#Set VLM to OpenAI model 
export VLM_MODEL_TO_USE=openai-compat
# gpt-4o
export VIA_VLM_OPENAI_MODEL_DEPLOYMENT_NAME=gpt-4o #FIX ME - change VLM model on remote endpoint
export VIA_VLM_ENDPOINT=https://api.openai.com/v1/chat/completions #FIX ME - change url to point to remote VLM endpoint. Can be any VLM with an openAI compatible API.
export NVIDIA_VISIBLE_DEVICES=1 


#Adjust misc configs if needed
export DISABLE_GUARDRAILS=true
#If you system has more than 1 GPU, configure which GPUs to use to run VSS. VSS can utilize multiple GPUs for lower latency summarization.


#custom configs for VLM video processing
export VLM_DEFAULT_NUM_FRAMES_PER_CHUNK=20
export NUM_VLM_PROCS=1             # VLM 프로세스 2개
export INSTALL_PROPRIETARY_CODECS=false